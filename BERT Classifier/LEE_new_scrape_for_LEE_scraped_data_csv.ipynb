{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "LEE_new_scarpe.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDycG6nsYaV0",
        "colab_type": "text"
      },
      "source": [
        "# Import necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9rU4LQGYaV2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "41a2f1dd-e513-4e48-d2b7-312aa3b35c1b"
      },
      "source": [
        "!pip install selenium\n",
        "#!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "\n",
        "import requests\n",
        "import bs4 as bs\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "import pandas as pd\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.6/dist-packages (3.141.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (83.0.4103.61-0ubuntu0.18.04.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 33 not upgraded.\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhlezpPEYaWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a new Chrome Session for Selenium\n",
        "options = webdriver.ChromeOptions()\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "driver = webdriver.Chrome('chromedriver',options=options)\n",
        "# driver = webdriver.Chrome(executable_path=\"./chromedriver_linux64/chromedr-iver\", options=options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO3UQ4f8tyuX",
        "colab_type": "text"
      },
      "source": [
        "# Creating scraped_data list that will contain all the data from all categories\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh4f2w_utxPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating scraped_data 2d list -> [ [url],[title],[content],[comment],[category] ]\n",
        "scraped_data = []\n",
        "# url will not be used to classify as it will obviously get 100% accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md8nKkvvYaWM",
        "colab_type": "text"
      },
      "source": [
        "# 1) Get 10 posts from T Replacement Category"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lSi4EuRYaV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Storing the url\n",
        "forum_feedback_url = 'https://forums.t-nation.com/c/t-replacement/'\n",
        "driver.implicitly_wait(2)\n",
        "driver.get(forum_feedback_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4t4p5SUYaWU",
        "colab_type": "text"
      },
      "source": [
        "## Scraping the page"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rkxm8yVYaWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scrolling down the category page\n",
        "\n",
        "for i in range(28): # scrolls down 28 times\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
        "    time.sleep(0.5)\n",
        "\n",
        "# Scraping the whole page \n",
        "page = driver.execute_script('return document.body.innerHTML')\n",
        "soup = bs.BeautifulSoup(''.join(page), 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMN7rvzyYaWa",
        "colab_type": "text"
      },
      "source": [
        "## Create and add category to 'scraped_data' dataframe "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcW_Wi-fi0ib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "posts = soup.find_all('a', class_= 'title raw-link raw-topic-link')\n",
        "\n",
        "for elem in posts[:834]:\n",
        "\n",
        "    driver.get(\"https://forums.t-nation.com\" + elem.get('href'))\n",
        "    time.sleep(1) #loading \n",
        "    page = driver.execute_script('return document.body.innerHTML')\n",
        "    soup = bs.BeautifulSoup(''.join(page),'html.parser')\n",
        "\n",
        "    # add content each row\n",
        "    all_content = soup.findAll('div', class_='cooked')\n",
        "\n",
        "    # boolean to manage exceptions\n",
        "    no_content = False\n",
        "    no_comment = False\n",
        "\n",
        "    # catching exception when there are no content captured\n",
        "    try:\n",
        "        content = all_content[0]\n",
        "    except:\n",
        "        # all_content.append('<div class=\"cooked\"><p></p></div>')\n",
        "        no_content = True\n",
        "        pass\n",
        "    \n",
        "    try:\n",
        "        comment = all_content[1] \n",
        "    except:\n",
        "        # all_content.append(all_content.text('<div class=\"cooked\"><p></p></div>'))\n",
        "        no_comment = True\n",
        "        pass\n",
        "\n",
        "    # add row to scraped_data\n",
        "    if no_content and no_comment:\n",
        "        scraped_data.append([elem.get('href'),elem.get_text(),'','','T Replacement'])\n",
        "    elif no_content:\n",
        "        scraped_data.append([elem.get('href'),elem.get_text(),'',comment.get_text(),'T Replacement'])\n",
        "    elif no_comment:\n",
        "        scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),'','T Replacement'])\n",
        "    else:\n",
        "        scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),comment.get_text(),'T Replacement'])\n",
        "     \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "# # adding [[url],[title],[],[]] to scraped_data\n",
        "# for elem in posts[:10]:\n",
        "#     scraped_data[0].append(elem.get('href'))\n",
        "#     scraped_data[1].append(elem.get_text())\n",
        "\n",
        "# # adding content and comments\n",
        "# for url in scraped_data[:][0]: # accessing 2d list\n",
        "#     try:\n",
        "#         driver.get(\"https://forums.t-nation.com\" + url)\n",
        "#         time.sleep(1) #loading \n",
        "#         page = driver.execute_script('return document.body.innerHTML')\n",
        "#         soup = bs.BeautifulSoup(''.join(page),'html.parser')\n",
        "        \n",
        "        \n",
        "\n",
        "#         # add content each row\n",
        "#         all_content = soup.findAll('div', class_='cooked')\n",
        "        \n",
        "#         content = all_content[0]\n",
        "# #        print('[0] ' + content.get_text())\n",
        "#         scraped_data[2].append(content.get_text())\n",
        "        \n",
        "#         # add comment each row\n",
        "# #        button = driver.find_element_by_id('widget-button btn-flat show-replies btn-icon-text')\n",
        "# #        button.click()\n",
        "# #        time.sleep(10)\n",
        "# #        div_container = soup.find('div', class_='topic-body')\n",
        "#         comment = all_content[1]\n",
        "# #        print('[1] ' + comment.get_text())\n",
        "#         scraped_data[3].append(content.get_text())\n",
        "#         scraped_data[4].append('Forum Feedback')\n",
        "#     except:\n",
        "#         pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRZUybW1s0BM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b92cd13-4bb2-42a5-ecf4-874752b3c540"
      },
      "source": [
        "len(posts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "870"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjFRQf3Fifbf",
        "colab_type": "text"
      },
      "source": [
        "# 2) Get 10 posts from Supplements and Nutrition Category \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9YMIgoHiEv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Storing the url\n",
        "supplement_nutrition_url = 'https://forums.t-nation.com/c/supplements-and-nutrition/'\n",
        "driver.implicitly_wait(2)\n",
        "driver.get(supplement_nutrition_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc_qy4wJkPsw",
        "colab_type": "text"
      },
      "source": [
        "## Scraping the page"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR2rdt5lkO3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scrolling down the category page\n",
        "\n",
        "for i in range(28): #scrolls down 28 times \n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
        "    time.sleep(0.5)\n",
        "\n",
        "# Scraping the whole page \n",
        "page = driver.execute_script('return document.body.innerHTML')\n",
        "soup = bs.BeautifulSoup(''.join(page), 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9USMi2VkfmF",
        "colab_type": "text"
      },
      "source": [
        "## Create and add category to 'scraped_data' dataframe "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uY9-KiLke4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "posts = soup.find_all('a', class_= 'title raw-link raw-topic-link')\n",
        "\n",
        "for elem in posts[:834]:\n",
        "\n",
        "    driver.get(\"https://forums.t-nation.com\" + elem.get('href'))\n",
        "    time.sleep(1) #loading \n",
        "    page = driver.execute_script('return document.body.innerHTML')\n",
        "    soup = bs.BeautifulSoup(''.join(page),'html.parser')\n",
        "\n",
        "    # add content each row\n",
        "    all_content = soup.findAll('div', class_='cooked')\n",
        "\n",
        "    # boolean to manage exceptions\n",
        "    no_content = False\n",
        "    no_comment = False\n",
        "\n",
        "    # catching exception when there are no content captured\n",
        "    try:\n",
        "        content = all_content[0]\n",
        "    except:\n",
        "        # all_content.append('<div class=\"cooked\"><p></p></div>')\n",
        "        no_content = True\n",
        "        pass\n",
        "    \n",
        "    try:\n",
        "        comment = all_content[1] \n",
        "    except:\n",
        "        # all_content.append(all_content.text('<div class=\"cooked\"><p></p></div>'))\n",
        "        no_comment = True\n",
        "        pass\n",
        "\n",
        "    # add row to scraped_data\n",
        "    if no_content and no_comment:\n",
        "        scraped_data.append([elem.get('href'),elem.get_text(),'','','Supplements and Nutrition'])\n",
        "    elif no_content:\n",
        "        scraped_data.append([elem.get('href'),elem.get_text(),'',comment.get_text(),'Supplements and Nutrition'])\n",
        "    elif no_comment:\n",
        "        scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),'','Supplements and Nutrition'])\n",
        "    else:\n",
        "        scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),comment.get_text(),'Supplements and Nutrition'])\n",
        "    \n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPXNC7vLu8To",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "89675a19-58b5-418d-8975-61d72c7407b1"
      },
      "source": [
        "len(posts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "840"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dsUWlW-Jl7vN"
      },
      "source": [
        "# 3) Get 10 posts from Training Log Category \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Kak1TeIRl7vQ",
        "colab": {}
      },
      "source": [
        "# Storing the url\n",
        "training_log_url = 'https://forums.t-nation.com/c/training-logs/'\n",
        "driver.implicitly_wait(2)\n",
        "driver.get(training_log_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q43m4iLTl7va"
      },
      "source": [
        "## Scraping the page"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Axv_HXo7l7vb",
        "colab": {}
      },
      "source": [
        "# Scrolling down the category page\n",
        "\n",
        "for i in range(28): #scrolls down 28 times \n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
        "    time.sleep(0.5)\n",
        "\n",
        "# Scraping the whole page \n",
        "page = driver.execute_script('return document.body.innerHTML')\n",
        "soup = bs.BeautifulSoup(''.join(page), 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fKL3hAe8l7vj"
      },
      "source": [
        "## Create and add category to 'scraped_data' dataframe "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wbdi7GNOl7vk",
        "colab": {}
      },
      "source": [
        "posts = soup.find_all('a', class_= 'title raw-link raw-topic-link')\n",
        "\n",
        "for elem in posts[:834]:\n",
        "\n",
        "    driver.get(\"https://forums.t-nation.com\" + elem.get('href'))\n",
        "    time.sleep(1) #loading \n",
        "    page = driver.execute_script('return document.body.innerHTML')\n",
        "    soup = bs.BeautifulSoup(''.join(page),'html.parser')\n",
        "\n",
        "    # add content each row\n",
        "    all_content = soup.findAll('div', class_='cooked')\n",
        "\n",
        "    # boolean to manage exceptions\n",
        "    no_content = False\n",
        "    no_comment = False\n",
        "\n",
        "    # catching exception when there are no content captured\n",
        "    try:\n",
        "        content = all_content[0]\n",
        "    except:\n",
        "        # all_content.append('<div class=\"cooked\"><p></p></div>')\n",
        "        no_content = True\n",
        "        pass\n",
        "    \n",
        "    try:\n",
        "        comment = all_content[1] \n",
        "    except:\n",
        "        # all_content.append(all_content.text('<div class=\"cooked\"><p></p></div>'))\n",
        "        no_comment = True\n",
        "        pass\n",
        "\n",
        "    # add row to scraped_data\n",
        "    if no_content and no_comment:\n",
        "        scraped_data.append([elem.get('href'),elem.get_text(),'','','Training Log'])\n",
        "    elif no_content:\n",
        "        scraped_data.append([elem.get('href'),elem.get_text(),'',comment.get_text(),'Training Log'])\n",
        "    elif no_comment:\n",
        "        scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),'','Training Log'])\n",
        "    else:\n",
        "        scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),comment.get_text(),'Training Log'])\n",
        "    \n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJe9rIZ3vnRH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e77d229-d835-4db4-8d5d-2df7dcedb9d8"
      },
      "source": [
        "len(posts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "870"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cblZGjocoHJU"
      },
      "source": [
        "# 4) Get 10 posts from Powerful Women Category \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "im-VWo7loHJW",
        "colab": {}
      },
      "source": [
        "# Storing the url\n",
        "powerful_women_url = 'https://forums.t-nation.com/c/powerful-women/'\n",
        "driver.implicitly_wait(2)\n",
        "driver.get(powerful_women_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SVKqRI7boHJb"
      },
      "source": [
        "## Scraping the page"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9esDS9GooHJc",
        "colab": {}
      },
      "source": [
        "# Scrolling down the category page\n",
        "\n",
        "for i in range(28): #scrolls down 28 times \n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
        "    time.sleep(0.5)\n",
        "\n",
        "# Scraping the whole page \n",
        "page = driver.execute_script('return document.body.innerHTML')\n",
        "soup = bs.BeautifulSoup(''.join(page), 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5QbgQhrkoHJi"
      },
      "source": [
        "## Create and add category to 'scraped_data' dataframe "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wi7UrS6JoHJj",
        "colab": {}
      },
      "source": [
        "posts = soup.find_all('a', class_= 'title raw-link raw-topic-link')\n",
        "\n",
        "for elem in posts[:834]:\n",
        "\n",
        "    driver.get(\"https://forums.t-nation.com\" + elem.get('href'))\n",
        "    time.sleep(1) #loading \n",
        "    page = driver.execute_script('return document.body.innerHTML')\n",
        "    soup = bs.BeautifulSoup(''.join(page),'html.parser')\n",
        "\n",
        "    # add content each row\n",
        "    all_content = soup.findAll('div', class_='cooked')\n",
        "\n",
        "    # boolean to manage exceptions\n",
        "    no_content = False\n",
        "    no_comment = False\n",
        "\n",
        "    # catching exception when there are no content captured\n",
        "    try:\n",
        "        content = all_content[0]\n",
        "    except:\n",
        "        # all_content.append('<div class=\"cooked\"><p></p></div>')\n",
        "        no_content = True\n",
        "        pass\n",
        "    \n",
        "    try:\n",
        "        comment = all_content[1] \n",
        "    except:\n",
        "        # all_content.append(all_content.text('<div class=\"cooked\"><p></p></div>'))\n",
        "        no_comment = True\n",
        "        pass\n",
        "\n",
        "    # add row to scraped_data\n",
        "    if no_content and no_comment:\n",
        "        scraped_data.append([elem.get('href'),elem.get_text(),'','','Powerful Women'])\n",
        "    elif no_content:\n",
        "        scraped_data.append([elem.get('href'),elem.get_text(),'',comment.get_text(),'Powerful Women'])\n",
        "    elif no_comment:\n",
        "        scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),'','Powerful Women'])\n",
        "    else:\n",
        "        scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),comment.get_text(),'Powerful Women'])\n",
        "    \n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjPzRn_uvyLr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29acc36e-46f1-4303-cc34-ddb630da4309"
      },
      "source": [
        "len(posts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "856"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GX39sQk2rhsZ"
      },
      "source": [
        "# 5) Get 10 posts from Combat Category \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bZRRIbF3rhsb",
        "colab": {}
      },
      "source": [
        "# Storing the url\n",
        "comabt_url = 'https://forums.t-nation.com/c/combat'\n",
        "driver.implicitly_wait(2)\n",
        "driver.get(comabt_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UO1NI-9_rhsf"
      },
      "source": [
        "## Scraping the page"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cPR3PIQLrhsf",
        "colab": {}
      },
      "source": [
        "# Scrolling down the category page\n",
        "\n",
        "for i in range(28): \n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
        "    time.sleep(0.5)\n",
        "\n",
        "# Scraping the whole page \n",
        "page = driver.execute_script('return document.body.innerHTML')\n",
        "soup = bs.BeautifulSoup(''.join(page), 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bbFpf6BCrhsj"
      },
      "source": [
        "## Create and add category to 'scraped_data' dataframe "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NftwRy2brhsj",
        "colab": {}
      },
      "source": [
        "posts = soup.find_all('a', class_= 'title raw-link raw-topic-link')\n",
        "\n",
        "for elem in posts[:834]:\n",
        "\n",
        "    driver.get(\"https://forums.t-nation.com\" + elem.get('href'))\n",
        "    time.sleep(1) #loading \n",
        "    page = driver.execute_script('return document.body.innerHTML')\n",
        "    soup = bs.BeautifulSoup(''.join(page),'html.parser')\n",
        "\n",
        "    # add content each row\n",
        "    all_content = soup.findAll('div', class_='cooked')\n",
        "\n",
        "    # boolean to manage exceptions\n",
        "    no_content = False\n",
        "    no_comment = False\n",
        "\n",
        "    # catching exception when there are no content captured\n",
        "    try:\n",
        "        content = all_content[0]\n",
        "    except:\n",
        "        # all_content.append('<div class=\"cooked\"><p></p></div>')\n",
        "        no_content = True\n",
        "        pass\n",
        "    \n",
        "    try:\n",
        "        comment = all_content[1] \n",
        "    except:\n",
        "        # all_content.append(all_content.text('<div class=\"cooked\"><p></p></div>'))\n",
        "        no_comment = True\n",
        "        pass\n",
        "\n",
        "    # add row to scraped_data\n",
        "    if no_content and no_comment:\n",
        "        scraped_data.append([elem.get('href'),elem.get_text(),'','','Combat'])\n",
        "    elif no_content:\n",
        "        scraped_data.append([elem.get('href'),elem.get_text(),'',comment.get_text(),'Combat'])\n",
        "    elif no_comment:\n",
        "        scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),'','Combat'])\n",
        "    else:\n",
        "        scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),comment.get_text(),'Combat'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c67tYec2v9MZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a4552371-8483-4a62-807f-3f85a341bf9b"
      },
      "source": [
        "len(posts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "840"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k1prOx_8rmZr"
      },
      "source": [
        "# 6) Get 10 posts from Injuries and Rehab Category \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v1MBsyVNrmZs",
        "colab": {}
      },
      "source": [
        "# Storing the url\n",
        "injuries_and_rehab_url = 'https://forums.t-nation.com/c/injuries-and-rehab/'\n",
        "driver.implicitly_wait(2)\n",
        "driver.get(injuries_and_rehab_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QBPAQ_RdrmZv"
      },
      "source": [
        "## Scraping the page"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I1SsVyk-rmZv",
        "colab": {}
      },
      "source": [
        "# Scrolling down the category page\n",
        "\n",
        "for i in range(28):\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
        "    time.sleep(0.5)\n",
        "\n",
        "# Scraping the whole page \n",
        "page = driver.execute_script('return document.body.innerHTML')\n",
        "soup = bs.BeautifulSoup(''.join(page), 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hX7Ws1d4rmZz"
      },
      "source": [
        "## Create and add category to 'scraped_data' dataframe "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WiU1h5MXrmZz",
        "colab": {}
      },
      "source": [
        "posts = soup.find_all('a', class_= 'title raw-link raw-topic-link')\n",
        "\n",
        "\n",
        "for elem in posts[:834]:\n",
        "\n",
        "    driver.get(\"https://forums.t-nation.com\" + elem.get('href'))\n",
        "    time.sleep(1) #loading \n",
        "    page = driver.execute_script('return document.body.innerHTML')\n",
        "    soup = bs.BeautifulSoup(''.join(page),'html.parser')\n",
        "\n",
        "    # add content each row\n",
        "    all_content = soup.findAll('div', class_='cooked')\n",
        "\n",
        "    # boolean to manage exceptions\n",
        "    no_content = False\n",
        "    no_comment = False\n",
        "\n",
        "    # catching exception when there are no content captured\n",
        "    try:\n",
        "        content = all_content[0]\n",
        "    except:\n",
        "        # all_content.append('<div class=\"cooked\"><p></p></div>')\n",
        "        no_content = True\n",
        "        pass\n",
        "    \n",
        "    try:\n",
        "        comment = all_content[1] \n",
        "    except:\n",
        "        # all_content.append(all_content.text('<div class=\"cooked\"><p></p></div>'))\n",
        "        no_comment = True\n",
        "        pass\n",
        "\n",
        "    # add row to scraped_data\n",
        "    if no_content and no_comment:\n",
        "        scraped_data.append([elem.get('href'),elem.get_text(),'','','Injuries and Rehab'])\n",
        "    elif no_content:\n",
        "        scraped_data.append([elem.get('href'),elem.get_text(),'',comment.get_text(),'Injuries and Rehab'])\n",
        "    elif no_comment:\n",
        "        scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),'','Injuries and Rehab'])\n",
        "    else:\n",
        "        scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),comment.get_text(),'Injuries and Rehab'])\n",
        "\n",
        "\n",
        "\n",
        "# for elem in posts[:10]: # specifying number of posts\n",
        "\n",
        "#     try:\n",
        "#         driver.get(\"https://forums.t-nation.com\" + elem.get('href'))\n",
        "#         time.sleep(1) #loading \n",
        "#         page = driver.execute_script('return document.body.innerHTML')\n",
        "#         soup = bs.BeautifulSoup(''.join(page),'html.parser')\n",
        "\n",
        "#         # add content each row\n",
        "#         all_content = soup.findAll('div', class_='cooked')\n",
        "\n",
        "#         content = all_content[0]\n",
        "#         comment = all_content[1]\n",
        "\n",
        "#         # add row to scraped_data\n",
        "#         scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),comment.get_text(),'Injuries and Rehab'])\n",
        "\n",
        "#     except:\n",
        "#        pass    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzW7ZjJowNKM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ee3244c-a4e3-418b-98d9-2e033f5867c2"
      },
      "source": [
        "len(posts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "840"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQu_tO6wpz_V",
        "colab_type": "text"
      },
      "source": [
        "# Creating Pandas dataframe from scraped_data list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crAIw7In4ugz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(scraped_data, columns=['url', 'title', 'content','comment','category'])\n",
        "df.to_csv(\"scraped_data.csv\", sep=',',index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LZdG-nH3GGH",
        "colab_type": "text"
      },
      "source": [
        "## Get rest of the posts and add to 'scraped_data' list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wABGKhHYaWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# posts = soup.find_all('a', class_= 'title raw-link raw-topic-link')\n",
        "# link = []\n",
        "# list_of_titles = []\n",
        "# for elem in posts:\n",
        "#     list_of_titles.append(elem.get_text())\n",
        "#     link.append(elem.get('href'))\n",
        "# length=len(list_of_titles)\n",
        "# list_of_titles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmdPGlyaYaWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# len(link)\n",
        "# link.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcgKuvibYaWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df = pd.DataFrame(data={\"col1\": link,\"col 2\":list_of_titles})\n",
        "# df.to_csv(\"file.csv\", sep=',',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8BPM-yWHsf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# checking the length of comments list\n",
        "# len(list_of_comments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRE6NFu-w-Cp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}