{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UDycG6nsYaV0"
   },
   "source": [
    "# Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "colab_type": "code",
    "id": "Z9rU4LQGYaV2",
    "outputId": "41a2f1dd-e513-4e48-d2b7-312aa3b35c1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /usr/local/lib/python3.6/dist-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "chromium-chromedriver is already the newest version (83.0.4103.61-0ubuntu0.18.04.1).\n",
      "The following package was automatically installed and is no longer required:\n",
      "  libnvidia-common-440\n",
      "Use 'apt autoremove' to remove it.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 33 not upgraded.\n",
      "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "#!apt-get update # to update ubuntu to correctly run apt install\n",
    "!apt install chromium-chromedriver\n",
    "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
    "import sys\n",
    "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
    "\n",
    "import requests\n",
    "import bs4 as bs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yhlezpPEYaWG"
   },
   "outputs": [],
   "source": [
    "# Creating a new Chrome Session for Selenium\n",
    "options = webdriver.ChromeOptions()\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "driver = webdriver.Chrome('chromedriver',options=options)\n",
    "# driver = webdriver.Chrome(executable_path=\"./chromedriver_linux64/chromedr-iver\", options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RO3UQ4f8tyuX"
   },
   "source": [
    "# Creating scraped_data list that will contain all the data from all categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wh4f2w_utxPB"
   },
   "outputs": [],
   "source": [
    "# creating scraped_data 2d list -> [ [url],[title],[content],[comment],[category] ]\n",
    "scraped_data = []\n",
    "# url will not be used to classify as it will obviously get 100% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "md8nKkvvYaWM"
   },
   "source": [
    "# 1) Get 10 posts from T Replacement Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6lSi4EuRYaV8"
   },
   "outputs": [],
   "source": [
    "# Storing the url\n",
    "forum_feedback_url = 'https://forums.t-nation.com/c/t-replacement/'\n",
    "driver.implicitly_wait(2)\n",
    "driver.get(forum_feedback_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K4t4p5SUYaWU"
   },
   "source": [
    "## Scraping the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Rkxm8yVYaWN"
   },
   "outputs": [],
   "source": [
    "# Scrolling down the category page\n",
    "\n",
    "for i in range(28): # scrolls down 28 times\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Scraping the whole page \n",
    "page = driver.execute_script('return document.body.innerHTML')\n",
    "soup = bs.BeautifulSoup(''.join(page), 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FMN7rvzyYaWa"
   },
   "source": [
    "## Create and add category to 'scraped_data' dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qcW_Wi-fi0ib"
   },
   "outputs": [],
   "source": [
    "posts = soup.find_all('a', class_= 'title raw-link raw-topic-link')\n",
    "\n",
    "for elem in posts[:834]:\n",
    "\n",
    "    driver.get(\"https://forums.t-nation.com\" + elem.get('href'))\n",
    "    time.sleep(1) #loading \n",
    "    page = driver.execute_script('return document.body.innerHTML')\n",
    "    soup = bs.BeautifulSoup(''.join(page),'html.parser')\n",
    "\n",
    "    # add content each row\n",
    "    all_content = soup.findAll('div', class_='cooked')\n",
    "\n",
    "    # boolean to manage exceptions\n",
    "    no_content = False\n",
    "    no_comment = False\n",
    "\n",
    "    # catching exception when there are no content captured\n",
    "    try:\n",
    "        content = all_content[0]\n",
    "    except:\n",
    "        # all_content.append('<div class=\"cooked\"><p></p></div>')\n",
    "        no_content = True\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        comment = all_content[1] \n",
    "    except:\n",
    "        # all_content.append(all_content.text('<div class=\"cooked\"><p></p></div>'))\n",
    "        no_comment = True\n",
    "        pass\n",
    "\n",
    "    # add row to scraped_data\n",
    "    if no_content and no_comment:\n",
    "        scraped_data.append([elem.get('href'),elem.get_text(),'','','T Replacement'])\n",
    "    elif no_content:\n",
    "        scraped_data.append([elem.get('href'),elem.get_text(),'',comment.get_text(),'T Replacement'])\n",
    "    elif no_comment:\n",
    "        scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),'','T Replacement'])\n",
    "    else:\n",
    "        scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),comment.get_text(),'T Replacement'])\n",
    "     \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# # adding [[url],[title],[],[]] to scraped_data\n",
    "# for elem in posts[:10]:\n",
    "#     scraped_data[0].append(elem.get('href'))\n",
    "#     scraped_data[1].append(elem.get_text())\n",
    "\n",
    "# # adding content and comments\n",
    "# for url in scraped_data[:][0]: # accessing 2d list\n",
    "#     try:\n",
    "#         driver.get(\"https://forums.t-nation.com\" + url)\n",
    "#         time.sleep(1) #loading \n",
    "#         page = driver.execute_script('return document.body.innerHTML')\n",
    "#         soup = bs.BeautifulSoup(''.join(page),'html.parser')\n",
    "        \n",
    "        \n",
    "\n",
    "#         # add content each row\n",
    "#         all_content = soup.findAll('div', class_='cooked')\n",
    "        \n",
    "#         content = all_content[0]\n",
    "# #        print('[0] ' + content.get_text())\n",
    "#         scraped_data[2].append(content.get_text())\n",
    "        \n",
    "#         # add comment each row\n",
    "# #        button = driver.find_element_by_id('widget-button btn-flat show-replies btn-icon-text')\n",
    "# #        button.click()\n",
    "# #        time.sleep(10)\n",
    "# #        div_container = soup.find('div', class_='topic-body')\n",
    "#         comment = all_content[1]\n",
    "# #        print('[1] ' + comment.get_text())\n",
    "#         scraped_data[3].append(content.get_text())\n",
    "#         scraped_data[4].append('Forum Feedback')\n",
    "#     except:\n",
    "#         pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TRZUybW1s0BM",
    "outputId": "6b92cd13-4bb2-42a5-ecf4-874752b3c540"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "870"
      ]
     },
     "execution_count": 124,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BjFRQf3Fifbf"
   },
   "source": [
    "# 2) Get 10 posts from Supplements and Nutrition Category \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o9YMIgoHiEv_"
   },
   "outputs": [],
   "source": [
    "# Storing the url\n",
    "supplement_nutrition_url = 'https://forums.t-nation.com/c/supplements-and-nutrition/'\n",
    "driver.implicitly_wait(2)\n",
    "driver.get(supplement_nutrition_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rc_qy4wJkPsw"
   },
   "source": [
    "## Scraping the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zR2rdt5lkO3K"
   },
   "outputs": [],
   "source": [
    "# Scrolling down the category page\n",
    "\n",
    "for i in range(28): #scrolls down 28 times \n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Scraping the whole page \n",
    "page = driver.execute_script('return document.body.innerHTML')\n",
    "soup = bs.BeautifulSoup(''.join(page), 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L9USMi2VkfmF"
   },
   "source": [
    "## Create and add category to 'scraped_data' dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7uY9-KiLke4V"
   },
   "outputs": [],
   "source": [
    "posts = soup.find_all('a', class_= 'title raw-link raw-topic-link')\n",
    "\n",
    "for elem in posts[:834]:\n",
    "\n",
    "    driver.get(\"https://forums.t-nation.com\" + elem.get('href'))\n",
    "    time.sleep(1) #loading \n",
    "    page = driver.execute_script('return document.body.innerHTML')\n",
    "    soup = bs.BeautifulSoup(''.join(page),'html.parser')\n",
    "\n",
    "    # add content each row\n",
    "    all_content = soup.findAll('div', class_='cooked')\n",
    "\n",
    "    # boolean to manage exceptions\n",
    "    no_content = False\n",
    "    no_comment = False\n",
    "\n",
    "    # catching exception when there are no content captured\n",
    "    try:\n",
    "        content = all_content[0]\n",
    "    except:\n",
    "        # all_content.append('<div class=\"cooked\"><p></p></div>')\n",
    "        no_content = True\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        comment = all_content[1] \n",
    "    except:\n",
    "        # all_content.append(all_content.text('<div class=\"cooked\"><p></p></div>'))\n",
    "        no_comment = True\n",
    "        pass\n",
    "\n",
    "    # add row to scraped_data\n",
    "    if no_content and no_comment:\n",
    "        scraped_data.append([elem.get('href'),elem.get_text(),'','','Supplements and Nutrition'])\n",
    "    elif no_content:\n",
    "        scraped_data.append([elem.get('href'),elem.get_text(),'',comment.get_text(),'Supplements and Nutrition'])\n",
    "    elif no_comment:\n",
    "        scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),'','Supplements and Nutrition'])\n",
    "    else:\n",
    "        scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),comment.get_text(),'Supplements and Nutrition'])\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KPXNC7vLu8To",
    "outputId": "89675a19-58b5-418d-8975-61d72c7407b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "840"
      ]
     },
     "execution_count": 128,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dsUWlW-Jl7vN"
   },
   "source": [
    "# 3) Get 10 posts from Training Log Category \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kak1TeIRl7vQ"
   },
   "outputs": [],
   "source": [
    "# Storing the url\n",
    "training_log_url = 'https://forums.t-nation.com/c/training-logs/'\n",
    "driver.implicitly_wait(2)\n",
    "driver.get(training_log_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q43m4iLTl7va"
   },
   "source": [
    "## Scraping the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Axv_HXo7l7vb"
   },
   "outputs": [],
   "source": [
    "# Scrolling down the category page\n",
    "\n",
    "for i in range(28): #scrolls down 28 times \n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Scraping the whole page \n",
    "page = driver.execute_script('return document.body.innerHTML')\n",
    "soup = bs.BeautifulSoup(''.join(page), 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fKL3hAe8l7vj"
   },
   "source": [
    "## Create and add category to 'scraped_data' dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wbdi7GNOl7vk"
   },
   "outputs": [],
   "source": [
    "posts = soup.find_all('a', class_= 'title raw-link raw-topic-link')\n",
    "\n",
    "for elem in posts[:834]:\n",
    "\n",
    "    driver.get(\"https://forums.t-nation.com\" + elem.get('href'))\n",
    "    time.sleep(1) #loading \n",
    "    page = driver.execute_script('return document.body.innerHTML')\n",
    "    soup = bs.BeautifulSoup(''.join(page),'html.parser')\n",
    "\n",
    "    # add content each row\n",
    "    all_content = soup.findAll('div', class_='cooked')\n",
    "\n",
    "    # boolean to manage exceptions\n",
    "    no_content = False\n",
    "    no_comment = False\n",
    "\n",
    "    # catching exception when there are no content captured\n",
    "    try:\n",
    "        content = all_content[0]\n",
    "    except:\n",
    "        # all_content.append('<div class=\"cooked\"><p></p></div>')\n",
    "        no_content = True\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        comment = all_content[1] \n",
    "    except:\n",
    "        # all_content.append(all_content.text('<div class=\"cooked\"><p></p></div>'))\n",
    "        no_comment = True\n",
    "        pass\n",
    "\n",
    "    # add row to scraped_data\n",
    "    if no_content and no_comment:\n",
    "        scraped_data.append([elem.get('href'),elem.get_text(),'','','Training Log'])\n",
    "    elif no_content:\n",
    "        scraped_data.append([elem.get('href'),elem.get_text(),'',comment.get_text(),'Training Log'])\n",
    "    elif no_comment:\n",
    "        scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),'','Training Log'])\n",
    "    else:\n",
    "        scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),comment.get_text(),'Training Log'])\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gJe9rIZ3vnRH",
    "outputId": "6e77d229-d835-4db4-8d5d-2df7dcedb9d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "870"
      ]
     },
     "execution_count": 132,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cblZGjocoHJU"
   },
   "source": [
    "# 4) Get 10 posts from Powerful Women Category \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "im-VWo7loHJW"
   },
   "outputs": [],
   "source": [
    "# Storing the url\n",
    "powerful_women_url = 'https://forums.t-nation.com/c/powerful-women/'\n",
    "driver.implicitly_wait(2)\n",
    "driver.get(powerful_women_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SVKqRI7boHJb"
   },
   "source": [
    "## Scraping the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9esDS9GooHJc"
   },
   "outputs": [],
   "source": [
    "# Scrolling down the category page\n",
    "\n",
    "for i in range(28): #scrolls down 28 times \n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Scraping the whole page \n",
    "page = driver.execute_script('return document.body.innerHTML')\n",
    "soup = bs.BeautifulSoup(''.join(page), 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5QbgQhrkoHJi"
   },
   "source": [
    "## Create and add category to 'scraped_data' dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wi7UrS6JoHJj"
   },
   "outputs": [],
   "source": [
    "posts = soup.find_all('a', class_= 'title raw-link raw-topic-link')\n",
    "\n",
    "for elem in posts[:834]:\n",
    "\n",
    "    driver.get(\"https://forums.t-nation.com\" + elem.get('href'))\n",
    "    time.sleep(1) #loading \n",
    "    page = driver.execute_script('return document.body.innerHTML')\n",
    "    soup = bs.BeautifulSoup(''.join(page),'html.parser')\n",
    "\n",
    "    # add content each row\n",
    "    all_content = soup.findAll('div', class_='cooked')\n",
    "\n",
    "    # boolean to manage exceptions\n",
    "    no_content = False\n",
    "    no_comment = False\n",
    "\n",
    "    # catching exception when there are no content captured\n",
    "    try:\n",
    "        content = all_content[0]\n",
    "    except:\n",
    "        # all_content.append('<div class=\"cooked\"><p></p></div>')\n",
    "        no_content = True\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        comment = all_content[1] \n",
    "    except:\n",
    "        # all_content.append(all_content.text('<div class=\"cooked\"><p></p></div>'))\n",
    "        no_comment = True\n",
    "        pass\n",
    "\n",
    "    # add row to scraped_data\n",
    "    if no_content and no_comment:\n",
    "        scraped_data.append([elem.get('href'),elem.get_text(),'','','Powerful Women'])\n",
    "    elif no_content:\n",
    "        scraped_data.append([elem.get('href'),elem.get_text(),'',comment.get_text(),'Powerful Women'])\n",
    "    elif no_comment:\n",
    "        scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),'','Powerful Women'])\n",
    "    else:\n",
    "        scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),comment.get_text(),'Powerful Women'])\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LjPzRn_uvyLr",
    "outputId": "29acc36e-46f1-4303-cc34-ddb630da4309"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "856"
      ]
     },
     "execution_count": 136,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GX39sQk2rhsZ"
   },
   "source": [
    "# 5) Get 10 posts from Combat Category \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bZRRIbF3rhsb"
   },
   "outputs": [],
   "source": [
    "# Storing the url\n",
    "comabt_url = 'https://forums.t-nation.com/c/combat'\n",
    "driver.implicitly_wait(2)\n",
    "driver.get(comabt_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UO1NI-9_rhsf"
   },
   "source": [
    "## Scraping the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cPR3PIQLrhsf"
   },
   "outputs": [],
   "source": [
    "# Scrolling down the category page\n",
    "\n",
    "for i in range(28): \n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Scraping the whole page \n",
    "page = driver.execute_script('return document.body.innerHTML')\n",
    "soup = bs.BeautifulSoup(''.join(page), 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbFpf6BCrhsj"
   },
   "source": [
    "## Create and add category to 'scraped_data' dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NftwRy2brhsj"
   },
   "outputs": [],
   "source": [
    "posts = soup.find_all('a', class_= 'title raw-link raw-topic-link')\n",
    "\n",
    "for elem in posts[:834]:\n",
    "\n",
    "    driver.get(\"https://forums.t-nation.com\" + elem.get('href'))\n",
    "    time.sleep(1) #loading \n",
    "    page = driver.execute_script('return document.body.innerHTML')\n",
    "    soup = bs.BeautifulSoup(''.join(page),'html.parser')\n",
    "\n",
    "    # add content each row\n",
    "    all_content = soup.findAll('div', class_='cooked')\n",
    "\n",
    "    # boolean to manage exceptions\n",
    "    no_content = False\n",
    "    no_comment = False\n",
    "\n",
    "    # catching exception when there are no content captured\n",
    "    try:\n",
    "        content = all_content[0]\n",
    "    except:\n",
    "        # all_content.append('<div class=\"cooked\"><p></p></div>')\n",
    "        no_content = True\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        comment = all_content[1] \n",
    "    except:\n",
    "        # all_content.append(all_content.text('<div class=\"cooked\"><p></p></div>'))\n",
    "        no_comment = True\n",
    "        pass\n",
    "\n",
    "    # add row to scraped_data\n",
    "    if no_content and no_comment:\n",
    "        scraped_data.append([elem.get('href'),elem.get_text(),'','','Combat'])\n",
    "    elif no_content:\n",
    "        scraped_data.append([elem.get('href'),elem.get_text(),'',comment.get_text(),'Combat'])\n",
    "    elif no_comment:\n",
    "        scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),'','Combat'])\n",
    "    else:\n",
    "        scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),comment.get_text(),'Combat'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c67tYec2v9MZ",
    "outputId": "a4552371-8483-4a62-807f-3f85a341bf9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "840"
      ]
     },
     "execution_count": 140,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k1prOx_8rmZr"
   },
   "source": [
    "# 6) Get 10 posts from Injuries and Rehab Category \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v1MBsyVNrmZs"
   },
   "outputs": [],
   "source": [
    "# Storing the url\n",
    "injuries_and_rehab_url = 'https://forums.t-nation.com/c/injuries-and-rehab/'\n",
    "driver.implicitly_wait(2)\n",
    "driver.get(injuries_and_rehab_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QBPAQ_RdrmZv"
   },
   "source": [
    "## Scraping the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I1SsVyk-rmZv"
   },
   "outputs": [],
   "source": [
    "# Scrolling down the category page\n",
    "\n",
    "for i in range(28):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Scraping the whole page \n",
    "page = driver.execute_script('return document.body.innerHTML')\n",
    "soup = bs.BeautifulSoup(''.join(page), 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hX7Ws1d4rmZz"
   },
   "source": [
    "## Create and add category to 'scraped_data' dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WiU1h5MXrmZz"
   },
   "outputs": [],
   "source": [
    "posts = soup.find_all('a', class_= 'title raw-link raw-topic-link')\n",
    "\n",
    "\n",
    "for elem in posts[:834]:\n",
    "\n",
    "    driver.get(\"https://forums.t-nation.com\" + elem.get('href'))\n",
    "    time.sleep(1) #loading \n",
    "    page = driver.execute_script('return document.body.innerHTML')\n",
    "    soup = bs.BeautifulSoup(''.join(page),'html.parser')\n",
    "\n",
    "    # add content each row\n",
    "    all_content = soup.findAll('div', class_='cooked')\n",
    "\n",
    "    # boolean to manage exceptions\n",
    "    no_content = False\n",
    "    no_comment = False\n",
    "\n",
    "    # catching exception when there are no content captured\n",
    "    try:\n",
    "        content = all_content[0]\n",
    "    except:\n",
    "        # all_content.append('<div class=\"cooked\"><p></p></div>')\n",
    "        no_content = True\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        comment = all_content[1] \n",
    "    except:\n",
    "        # all_content.append(all_content.text('<div class=\"cooked\"><p></p></div>'))\n",
    "        no_comment = True\n",
    "        pass\n",
    "\n",
    "    # add row to scraped_data\n",
    "    if no_content and no_comment:\n",
    "        scraped_data.append([elem.get('href'),elem.get_text(),'','','Injuries and Rehab'])\n",
    "    elif no_content:\n",
    "        scraped_data.append([elem.get('href'),elem.get_text(),'',comment.get_text(),'Injuries and Rehab'])\n",
    "    elif no_comment:\n",
    "        scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),'','Injuries and Rehab'])\n",
    "    else:\n",
    "        scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),comment.get_text(),'Injuries and Rehab'])\n",
    "\n",
    "\n",
    "\n",
    "# for elem in posts[:10]: # specifying number of posts\n",
    "\n",
    "#     try:\n",
    "#         driver.get(\"https://forums.t-nation.com\" + elem.get('href'))\n",
    "#         time.sleep(1) #loading \n",
    "#         page = driver.execute_script('return document.body.innerHTML')\n",
    "#         soup = bs.BeautifulSoup(''.join(page),'html.parser')\n",
    "\n",
    "#         # add content each row\n",
    "#         all_content = soup.findAll('div', class_='cooked')\n",
    "\n",
    "#         content = all_content[0]\n",
    "#         comment = all_content[1]\n",
    "\n",
    "#         # add row to scraped_data\n",
    "#         scraped_data.append([elem.get('href'),elem.get_text(),content.get_text(),comment.get_text(),'Injuries and Rehab'])\n",
    "\n",
    "#     except:\n",
    "#        pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TzW7ZjJowNKM",
    "outputId": "3ee3244c-a4e3-418b-98d9-2e033f5867c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "840"
      ]
     },
     "execution_count": 144,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hQu_tO6wpz_V"
   },
   "source": [
    "# Creating Pandas dataframe from scraped_data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "crAIw7In4ugz"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(scraped_data, columns=['url', 'title', 'content','comment','category'])\n",
    "df.to_csv(\"LEE_scraped_data.csv\", sep=',',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6LZdG-nH3GGH"
   },
   "source": [
    "## Get rest of the posts and add to 'scraped_data' list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_wABGKhHYaWc"
   },
   "outputs": [],
   "source": [
    "# posts = soup.find_all('a', class_= 'title raw-link raw-topic-link')\n",
    "# link = []\n",
    "# list_of_titles = []\n",
    "# for elem in posts:\n",
    "#     list_of_titles.append(elem.get_text())\n",
    "#     link.append(elem.get('href'))\n",
    "# length=len(list_of_titles)\n",
    "# list_of_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RmdPGlyaYaWl"
   },
   "outputs": [],
   "source": [
    "\n",
    "# len(link)\n",
    "# link.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcgKuvibYaWr"
   },
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(data={\"col1\": link,\"col 2\":list_of_titles})\n",
    "# df.to_csv(\"file.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i8BPM-yWHsf6"
   },
   "outputs": [],
   "source": [
    "# checking the length of comments list\n",
    "# len(list_of_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lRE6NFu-w-Cp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LEE_new_scarpe.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
